{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Reshape, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mtcnn import MTCNN\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe face mesh (for training and detection)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN (only for registration)\n",
    "mtcnn_detector = MTCNN()\n",
    "\n",
    "def check_user_exists(user_id, dataset_dir):\n",
    "    \"\"\"Check if user already exists in dataset with enhanced error message\"\"\"\n",
    "    user_folder = os.path.join(dataset_dir, user_id)\n",
    "    exists = os.path.exists(user_folder)\n",
    "    \n",
    "    if exists:\n",
    "        print(\"\\n\" + \"!\"*50)\n",
    "        print(f\" PERINGATAN: NIM {user_id} sudah terdaftar! \")\n",
    "        print(\"!\"*50)\n",
    "        print(f\"\\nDirektori yang sudah ada: {user_folder}\")\n",
    "        print(\"\\nSilakan pilih:\")\n",
    "        print(\"1. Gunakan NIM berbeda\")\n",
    "        print(\"2. Hapus folder tersebut jika ingin mendaftar ulang\")\n",
    "        print(\"!\"*50 + \"\\n\")\n",
    "    \n",
    "    return exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_user_with_mtcnn(user_id, dataset_dir):\n",
    "    \"\"\"Register new user using MTCNN for face detection\"\"\"\n",
    "    user_folder = os.path.join(dataset_dir, user_id)\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "    \n",
    "    # Ask if user is wearing glasses\n",
    "    wear_glasses = input(\"Are you wearing glasses? (y/n): \").lower() == 'y'\n",
    "    num_images = 12 if wear_glasses else 10  # 12 images if wearing glasses, 10 if not\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(f\"Registering user {user_id}. Please position your face in the frame.\")\n",
    "    print(f\"Will capture {num_images} images.\")\n",
    "    \n",
    "    captured_images = 0\n",
    "    while captured_images < num_images and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Detect faces with MTCNN\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mtcnn_detector.detect_faces(frame_rgb)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            # Get the largest face\n",
    "            x, y, w, h = results[0]['box']\n",
    "            face = frame_rgb[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, (160, 160))\n",
    "            \n",
    "            # Save image\n",
    "            img_path = os.path.join(user_folder, f\"{user_id}_{captured_images}.jpg\")\n",
    "            cv2.imwrite(img_path, cv2.cvtColor(face_resized, cv2.COLOR_RGB2BGR))\n",
    "            captured_images += 1\n",
    "            print(f\"Captured image {captured_images}/{num_images}\")\n",
    "            \n",
    "            # Show countdown\n",
    "            for i in range(3, 0, -1):\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    cv2.putText(frame, f\"Next in {i}...\", (10, 30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.imshow('Registration', frame)\n",
    "                    cv2.waitKey(1000)\n",
    "        \n",
    "        # Display instructions\n",
    "        cv2.putText(frame, f\"Captured: {captured_images}/{num_images}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.imshow('Registration', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Registration complete for {user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mediapipe_landmarks(image):\n",
    "    \"\"\"Extract face landmarks using MediaPipe\"\"\"\n",
    "    results = face_mesh.process(image)\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = []\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "        return np.array(landmarks)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mediapipe(dataset_dir):\n",
    "    \"\"\"Load dataset using MediaPipe for landmark extraction\"\"\"\n",
    "    X, y = [], []\n",
    "    print(f\"Loading dataset from {dataset_dir}\")\n",
    "    \n",
    "    for label in os.listdir(dataset_dir):\n",
    "        label_path = os.path.join(dataset_dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {label}...\")\n",
    "        for img_name in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_name)\n",
    "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "                \n",
    "            # Read and process image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            landmarks = extract_mediapipe_landmarks(image_rgb) \n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X.append(landmarks)\n",
    "                y.append(label)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} samples\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build a simple neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_and_landmarks(image_rgb):\n",
    "    \"\"\"Extract face and landmarks using MediaPipe\"\"\"\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        # Get face bounding box from landmarks\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        xs = [lm.x for lm in landmarks]\n",
    "        ys = [lm.y for lm in landmarks]\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        x_min, x_max = min(xs), max(xs)\n",
    "        y_min, y_max = min(ys), max(ys)\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        h, w, _ = image_rgb.shape\n",
    "        x1, y1 = int(x_min * w), int(y_min * h)\n",
    "        x2, y2 = int(x_max * w), int(y_max * h)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmark_features = []\n",
    "        for landmark in landmarks:\n",
    "            landmark_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        return np.array(landmark_features), (x1, y1, x2-x1, y2-y1)\n",
    "    \n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime_detection_with_mediapipe(model, label_encoder):\n",
    "    \"\"\"Real-time face recognition using MediaPipe\"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_map = {i: name for i, name in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Start timing for FPS calculation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get face landmarks\n",
    "        landmarks, bbox = extract_face_and_landmarks(frame_rgb)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            # Predict\n",
    "            features = landmarks.reshape(1, -1)\n",
    "            predictions = model.predict(features, verbose=0)\n",
    "            idx = np.argmax(predictions)\n",
    "            confidence = np.max(predictions) * 100\n",
    "            \n",
    "            # Get label\n",
    "            label = label_map.get(idx, \"Unknown\")\n",
    "            \n",
    "            # Draw results\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.1f}%)\", (x, y - 10),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "        \n",
    "        fps_color = (0, 255, 0) if fps > 15 else (0, 255, 255) if fps > 10 else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, fps_color, 2)\n",
    "        \n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering user 232232222222. Please position your face in the frame.\n",
      "Will capture 10 images.\n",
      "Captured image 1/10\n",
      "Captured image 2/10\n",
      "Captured image 3/10\n",
      "Captured image 4/10\n",
      "Captured image 5/10\n",
      "Captured image 6/10\n",
      "Captured image 7/10\n",
      "Captured image 8/10\n",
      "Captured image 9/10\n",
      "Captured image 10/10\n",
      "Registration complete for 232232222222\n",
      "Loading dataset from ./dataset_model_skripsi/\n",
      "Processing 2123231212...\n",
      "Processing 232232222222...\n",
      "Loaded 20 samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 256)               367360    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 410,178\n",
      "Trainable params: 409,410\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 837ms/step - loss: 8.1286 - accuracy: 0.1250 - val_loss: 6.6239 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.4538 - accuracy: 0.6875 - val_loss: 6.5635 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.1427 - accuracy: 0.8125 - val_loss: 6.5023 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.9084 - accuracy: 1.0000 - val_loss: 6.4154 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.8736 - accuracy: 0.8750 - val_loss: 6.3435 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.7108 - accuracy: 1.0000 - val_loss: 6.2698 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.7952 - accuracy: 0.8125 - val_loss: 6.1688 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.7434 - accuracy: 0.8125 - val_loss: 6.0879 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.5935 - accuracy: 0.8125 - val_loss: 6.0270 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.5126 - accuracy: 0.7500 - val_loss: 5.9690 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.3399 - accuracy: 0.8750 - val_loss: 5.9210 - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.1023 - accuracy: 0.8750 - val_loss: 5.8838 - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.9313 - accuracy: 0.9375 - val_loss: 5.8569 - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.1114 - accuracy: 0.8125 - val_loss: 5.8432 - val_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.7913 - accuracy: 0.9375 - val_loss: 5.8156 - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.6538 - accuracy: 1.0000 - val_loss: 5.7928 - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.6041 - accuracy: 0.9375 - val_loss: 5.7645 - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.5272 - accuracy: 0.9375 - val_loss: 5.7326 - val_accuracy: 0.5000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.5571 - accuracy: 0.8750 - val_loss: 5.6931 - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.4038 - accuracy: 0.9375 - val_loss: 5.6439 - val_accuracy: 0.5000\n",
      "Model trained and saved.\n",
      "Test accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = './dataset_model_skripsi/'\n",
    "    \n",
    "    # User registration (using MTCNN)\n",
    "    register_new = input(\"Register new user? (y/n): \").lower()\n",
    "    if register_new == 'y':\n",
    "        user_id = input(\"Enter user ID/NIM: \")\n",
    "        \n",
    "        # Check if user already exists\n",
    "        if check_user_exists(user_id, dataset_dir):\n",
    "            print(f\"Error: User {user_id} already exists in the dataset!\")\n",
    "            print(\"Please use a different NIM or delete the existing folder if you want to re-register.\")\n",
    "        else:\n",
    "            register_user_with_mtcnn(user_id, dataset_dir)\n",
    "    \n",
    "    # Load dataset (using MediaPipe)\n",
    "    X, y = load_dataset_with_mediapipe(dataset_dir)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Hitung class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_model((X.shape[1],), len(label_encoder.classes_))\n",
    "    model.fit(X_train, y_train, epochs=20, class_weight=class_weights, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Save model\n",
    "    model.save('face_recognition_mediapipe.h5')\n",
    "    print(\"Model trained and saved.\")\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Real-time detection (using MediaPipe)\n",
    "    realtime_detection_with_mediapipe(model, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.Series(y).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
