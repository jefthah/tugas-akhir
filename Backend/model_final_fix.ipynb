{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Reshape, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mtcnn import MTCNN\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe face mesh (for training and detection)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN (only for registration)\n",
    "mtcnn_detector = MTCNN()\n",
    "\n",
    "def check_user_exists(user_id, dataset_dir):\n",
    "    \"\"\"Check if user already exists in dataset with enhanced error message\"\"\"\n",
    "    user_folder = os.path.join(dataset_dir, user_id)\n",
    "    exists = os.path.exists(user_folder)\n",
    "    \n",
    "    if exists:\n",
    "        print(\"\\n\" + \"!\"*50)\n",
    "        print(f\" PERINGATAN: NIM {user_id} sudah terdaftar! \")\n",
    "        print(\"!\"*50)\n",
    "        print(f\"\\nDirektori yang sudah ada: {user_folder}\")\n",
    "        print(\"\\nSilakan pilih:\")\n",
    "        print(\"1. Gunakan NIM berbeda\")\n",
    "        print(\"2. Hapus folder tersebut jika ingin mendaftar ulang\")\n",
    "        print(\"!\"*50 + \"\\n\")\n",
    "    \n",
    "    return exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mediapipe_landmarks(image):\n",
    "    \"\"\"Extract face landmarks using MediaPipe\"\"\"\n",
    "    results = face_mesh.process(image)\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = []\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "        return np.array(landmarks)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mediapipe(dataset_dir):\n",
    "    \"\"\"Load dataset using MediaPipe for landmark extraction\"\"\"\n",
    "    X, y = [], []\n",
    "    print(f\"Loading dataset from {dataset_dir}\")\n",
    "    \n",
    "    for label in os.listdir(dataset_dir):\n",
    "        label_path = os.path.join(dataset_dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {label}...\")\n",
    "        for img_name in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_name)\n",
    "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "                \n",
    "            # Read and process image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            landmarks = extract_mediapipe_landmarks(image_rgb) \n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X.append(landmarks)\n",
    "                y.append(label)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} samples\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_face_already_registered(face_image, dataset_dir, threshold=0.8):\n",
    "    \"\"\"Check if the face already exists in the dataset using face comparison\"\"\"\n",
    "    # Load existing dataset\n",
    "    X, y = load_dataset_with_mediapipe(dataset_dir)\n",
    "    if len(X) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Extract landmarks from new face\n",
    "    new_landmarks = extract_mediapipe_landmarks(face_image)\n",
    "    if new_landmarks is None:\n",
    "        return False\n",
    "    \n",
    "    # Compare with all existing faces\n",
    "    for existing_landmarks in X:\n",
    "        similarity = np.dot(new_landmarks, existing_landmarks) / (\n",
    "            np.linalg.norm(new_landmarks) * np.linalg.norm(existing_landmarks))\n",
    "        if similarity > threshold:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_user_with_verification(user_id, dataset_dir):\n",
    "    \"\"\"Register new user with both NIM and face verification\"\"\"\n",
    "    # First check NIM\n",
    "    if check_user_exists(user_id, dataset_dir):\n",
    "        return False\n",
    "    \n",
    "    # Then proceed with face registration and check\n",
    "    user_folder = os.path.join(dataset_dir, user_id)\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "    \n",
    "    wear_glasses = input(\"Apakah Anda menggunakan kacamata? (y/n): \").lower() == 'y'\n",
    "    num_images = 12 if wear_glasses else 10\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(f\"\\nRegistrasi user {user_id}. Harap posisikan wajah Anda di frame.\")\n",
    "    print(f\"Akan mengambil {num_images} gambar.\")\n",
    "    \n",
    "    captured_images = 0\n",
    "    face_already_registered = False\n",
    "    \n",
    "    while captured_images < num_images and cap.isOpened() and not face_already_registered:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mtcnn_detector.detect_faces(frame_rgb)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            x, y, w, h = results[0]['box']\n",
    "            face = frame_rgb[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, (160, 160))\n",
    "            \n",
    "            # Check for duplicate face only on first capture\n",
    "            if captured_images == 0:\n",
    "                if is_face_already_registered(face_resized, dataset_dir):\n",
    "                    print(\"\\n\" + \"!\"*50)\n",
    "                    print(\" ERROR: WAJAH SUDAH TERDAFTAR! \")\n",
    "                    print(\"!\"*50)\n",
    "                    print(\"\\nAlasan tidak bisa mendaftarkan:\")\n",
    "                    print(\"- Wajah ini sudah terdaftar dengan NIM berbeda\")\n",
    "                    print(\"- Sistem mendeteksi kemiripan wajah >80% dengan data existing\")\n",
    "                    print(\"\\nSolusi:\")\n",
    "                    print(\"- Gunakan akun yang sudah terdaftar jika ini memang Anda\")\n",
    "                    print(\"- Hubungi admin jika ini kesalahan sistem\")\n",
    "                    print(\"!\"*50 + \"\\n\")\n",
    "                    face_already_registered = True\n",
    "                    break\n",
    "            \n",
    "            if not face_already_registered:\n",
    "                img_path = os.path.join(user_folder, f\"{user_id}_{captured_images}.jpg\")\n",
    "                cv2.imwrite(img_path, cv2.cvtColor(face_resized, cv2.COLOR_RGB2BGR))\n",
    "                captured_images += 1\n",
    "                print(f\"Gambar terambil {captured_images}/{num_images}\")\n",
    "                \n",
    "                for i in range(3, 0, -1):\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        cv2.putText(frame, f\"Berikutnya dalam {i}...\", (10, 30), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                        cv2.imshow('Registrasi', frame)\n",
    "                        cv2.waitKey(1000)\n",
    "        \n",
    "        cv2.putText(frame, f\"Terekam: {captured_images}/{num_images}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.imshow('Registrasi', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    if face_already_registered:\n",
    "        # Cleanup - remove the user folder if face was duplicate\n",
    "        import shutil\n",
    "        shutil.rmtree(user_folder)\n",
    "        print(f\"Registrasi dibatalkan untuk {user_id} - wajah sudah terdaftar\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Registrasi berhasil untuk {user_id}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build a simple neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_and_landmarks(image_rgb):\n",
    "    \"\"\"Extract face and landmarks using MediaPipe\"\"\"\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        # Get face bounding box from landmarks\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        xs = [lm.x for lm in landmarks]\n",
    "        ys = [lm.y for lm in landmarks]\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        x_min, x_max = min(xs), max(xs)\n",
    "        y_min, y_max = min(ys), max(ys)\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        h, w, _ = image_rgb.shape\n",
    "        x1, y1 = int(x_min * w), int(y_min * h)\n",
    "        x2, y2 = int(x_max * w), int(y_max * h)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmark_features = []\n",
    "        for landmark in landmarks:\n",
    "            landmark_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        return np.array(landmark_features), (x1, y1, x2-x1, y2-y1)\n",
    "    \n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime_detection_with_mediapipe(model, label_encoder):\n",
    "    \"\"\"Real-time face recognition using MediaPipe\"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_map = {i: name for i, name in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Start timing for FPS calculation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get face landmarks\n",
    "        landmarks, bbox = extract_face_and_landmarks(frame_rgb)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            # Predict\n",
    "            features = landmarks.reshape(1, -1)\n",
    "            predictions = model.predict(features, verbose=0)\n",
    "            idx = np.argmax(predictions)\n",
    "            confidence = np.max(predictions) * 100\n",
    "            \n",
    "            # Get label\n",
    "            label = label_map.get(idx, \"Unknown\")\n",
    "            \n",
    "            # Draw results\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.1f}%)\", (x, y - 10),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "        \n",
    "        fps_color = (0, 255, 0) if fps > 15 else (0, 255, 255) if fps > 10 else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, fps_color, 2)\n",
    "        \n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from C:/Users/jefta/Documents/Tugas Akhir Next js/prototype-leads/Backend/dataset_model_skripsi/\n",
      "Processing 2222222222...\n",
      "Loaded 5 samples\n",
      "Shape of y_train: (4, 2)\n",
      "Shape of y_test: (1, 2)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               367360    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 410,178\n",
      "Trainable params: 409,410\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.4262 - accuracy: 0.0000e+00 - val_loss: 6.5250 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.1591 - accuracy: 0.0000e+00 - val_loss: 6.3600 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.9822 - accuracy: 0.5000 - val_loss: 6.2318 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.7693 - accuracy: 0.5000 - val_loss: 6.0157 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.2989 - accuracy: 0.5000 - val_loss: 5.8156 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.1066 - accuracy: 0.5000 - val_loss: 5.6384 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9024 - accuracy: 0.7500 - val_loss: 5.4982 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.9656 - accuracy: 0.5000 - val_loss: 5.3818 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.6600 - accuracy: 0.7500 - val_loss: 5.2614 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.4616 - accuracy: 0.7500 - val_loss: 5.1389 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.6112 - accuracy: 0.5000 - val_loss: 5.0340 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.1661 - accuracy: 1.0000 - val_loss: 4.9285 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.3481 - accuracy: 0.5000 - val_loss: 4.8223 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.8963 - accuracy: 1.0000 - val_loss: 4.7196 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.9633 - accuracy: 0.7500 - val_loss: 4.6255 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.8999 - accuracy: 0.7500 - val_loss: 4.5406 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5977 - accuracy: 1.0000 - val_loss: 4.4603 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.4244 - accuracy: 1.0000 - val_loss: 4.3837 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.4650 - accuracy: 1.0000 - val_loss: 4.3093 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.5185 - accuracy: 0.7500 - val_loss: 4.2383 - val_accuracy: 1.0000\n",
      "Model trained and saved.\n",
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = 'C:/Users/jefta/Documents/Tugas Akhir Next js/prototype-leads/Backend/dataset_model_skripsi/'\n",
    "\n",
    "    # User registration (using MTCNN)\n",
    "    register_new = input(\"Register new user? (y/n): \").lower()\n",
    "    if register_new == 'y':\n",
    "        user_id = input(\"Enter user ID/NIM: \")\n",
    "        \n",
    "        # Check if user already exists\n",
    "        if check_user_exists(user_id, dataset_dir):\n",
    "            print(f\"Error: User {user_id} already exists in the dataset!\")\n",
    "            print(\"Please use a different NIM or delete the existing folder if you want to re-register.\")\n",
    "        else:\n",
    "            register_user_with_verification(user_id, dataset_dir)\n",
    "    \n",
    "    # Load dataset (using MediaPipe)\n",
    "    X, y = load_dataset_with_mediapipe(dataset_dir)\n",
    "    \n",
    "    # Encode labels (convert to one-hot encoding for categorical crossentropy)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Encode labels\n",
    "    y_categorical = to_categorical(y_encoded, num_classes=2)  # One-hot encoding for categorical crossentropy\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Check shapes of y_train and y_test to confirm one-hot encoding\n",
    "    print(\"Shape of y_train:\", y_train.shape)  # Should be (8, 2) if two classes and one-hot encoding\n",
    "    print(\"Shape of y_test:\", y_test.shape)  # Should be (2, 2) if two classes and one-hot encoding\n",
    "\n",
    "    # Compute class weights (handle imbalanced classes)\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "    class_weights = dict(enumerate(class_weights))  # Convert to a dictionary for use in model fitting\n",
    "\n",
    "    # Build and train model (use output units equal to the number of classes)\n",
    "    model = build_model((X.shape[1],), 2)  # Set number of output units to 2 for binary classification with categorical crossentropy\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=20, class_weight=class_weights, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Save model\n",
    "    model.save('face_recognition_mediapipe.h5')\n",
    "    print(\"Model trained and saved.\")\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Real-time detection (using MediaPipe)\n",
    "    realtime_detection_with_mediapipe(model, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222222222    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.Series(y).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
